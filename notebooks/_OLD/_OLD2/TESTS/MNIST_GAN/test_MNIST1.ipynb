{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f816e65",
   "metadata": {},
   "source": [
    "# Idea general (qu√© est√°s implementando)\n",
    "\n",
    "Vamos a implementar la estructura cl√°sica de una GAN:\n",
    "\n",
    "- Generador ùê∫(z)\n",
    "\t- Entrada: vector de ruido z ‚àà R^100 (distribuci√≥n normal o uniforme).\n",
    "\t- Salida: imagen 1√ó28√ó28 que ‚Äúparece‚Äù MNIST.\n",
    "\t- Arquitectura: Linear ‚Üí reshape ‚Üí ConvTranspose2d ‚Üí ConvTranspose2d ‚Üí Tanh.\n",
    "\n",
    "- Discriminador ùê∑(x)\n",
    "\t- Entrada: imagen real o generada x ‚àà R^{1√ó28√ó28}.\n",
    "\t- Salida: probabilidad D(x) ‚àà (0,1) de que la imagen sea real.\n",
    "\t- Arquitectura: Conv2d ‚Üí Conv2d ‚Üí Linear ‚Üí Sigmoid.\n",
    "\n",
    "Funci√≥n de p√©rdida:\n",
    "\n",
    "Discriminador:\n",
    "\n",
    "$$ùêø_{ùê∑} = ‚àíùê∏_{ùë•‚àºùëù_{data}}[logùê∑(ùë•)] ‚àí ùê∏_{ùëß‚àºùëù_{ùëß}}[log(1‚àíùê∑(ùê∫(ùëß)))]$$\n",
    "\t‚Äã\n",
    "\n",
    "\n",
    "\n",
    "Generador:\n",
    "\n",
    "$$ùêø_{G} = ‚àíùê∏_{z~p_{z}}[logD(G(z))]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548bc1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0d41230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 1. Hyperpar√°metros\n",
    "# ===========================\n",
    "latent_dim = 100       # dimensi√≥n del vector de ruido z\n",
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "lr = 2e-4\n",
    "beta1 = 0.5            # para Adam, t√≠pico en DCGAN\n",
    "\n",
    "image_size = 28\n",
    "image_channels = 1     # MNIST es en escala de grises\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando dispositivo:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bda2a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carpeta para guardar resultados\n",
    "os.makedirs(\"samples\", exist_ok=True)\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db272851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:01<00:00, 8.55MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 289kB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:00<00:00, 2.76MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 27.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 2. Dataset y DataLoader\n",
    "# ===========================\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # Normalizamos a media 0 y varianza 1 en [-1, 1] (para Tanh en la salida del generador)\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9700db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "439a132c",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image tensor: torch.Size([1, 28, 28]), Label: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7172c4d5d3d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aux1 = train_loader.dataset.__getitem__(0)  # Verificar que el dataset carga correctamente\n",
    "print(f\"Shape of image tensor: {aux1[0].shape}, Label: {aux1[1]}\")\n",
    "plt.imshow(aux1[0].squeeze_(0), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "465bf042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 3. Definici√≥n del Generador\n",
    "# ===========================\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generador G: mapea un vector de ruido z (latent_dim) a una imagen 1x28x28.\n",
    "    Usamos una combinaci√≥n de capas lineales y convoluciones transpuestas.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256 * 7 * 7),\n",
    "            nn.BatchNorm1d(256 * 7 * 7),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # Entrada: (256, 7, 7)\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # -> (128, 14, 14)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, image_channels, kernel_size=4, stride=2, padding=1),  # -> (1, 28, 28)\n",
    "            nn.Tanh()  # salida en [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # z: (batch_size, latent_dim)\n",
    "        x = self.fc(z)  # (batch_size, 256*7*7)\n",
    "        x = x.view(-1, 256, 7, 7)  # reshape a (batch_size, 256, 7, 7)\n",
    "        img = self.conv_blocks(x)  # (batch_size, 1, 28, 28)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07234164",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===========================\n",
    "# 4. Definici√≥n del Discriminador\n",
    "# ===========================\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminador D: recibe una imagen 1x28x28 y devuelve probabilidad de ser real.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # Entrada: (1, 28, 28)\n",
    "            nn.Conv2d(image_channels, 64, kernel_size=4, stride=2, padding=1),  # -> (64, 14, 14)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # -> (128, 7, 7)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 7 * 7, 1),\n",
    "            nn.Sigmoid()  # probabilidad de \"real\"\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.conv_blocks(img)\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "107c230b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Lote [0/469] Loss D: 2.2394 Loss G: 1.4114\n",
      "Epoch [1/20] Lote [200/469] Loss D: 0.6591 Loss G: 1.1874\n",
      "Epoch [1/20] Lote [400/469] Loss D: 0.6742 Loss G: 0.8961\n",
      "Epoch [2/20] Lote [0/469] Loss D: 0.5210 Loss G: 1.1476\n",
      "Epoch [2/20] Lote [200/469] Loss D: 0.4469 Loss G: 1.4531\n",
      "Epoch [2/20] Lote [400/469] Loss D: 0.4902 Loss G: 2.1091\n",
      "Epoch [3/20] Lote [0/469] Loss D: 0.5708 Loss G: 1.3695\n",
      "Epoch [3/20] Lote [200/469] Loss D: 0.5920 Loss G: 2.6707\n",
      "Epoch [3/20] Lote [400/469] Loss D: 0.6365 Loss G: 1.7608\n",
      "Epoch [4/20] Lote [0/469] Loss D: 0.6961 Loss G: 1.9670\n",
      "Epoch [4/20] Lote [200/469] Loss D: 0.6065 Loss G: 1.6875\n",
      "Epoch [4/20] Lote [400/469] Loss D: 0.7979 Loss G: 0.8164\n",
      "Epoch [5/20] Lote [0/469] Loss D: 0.6678 Loss G: 1.6995\n",
      "Epoch [5/20] Lote [200/469] Loss D: 0.6442 Loss G: 1.1861\n",
      "Epoch [5/20] Lote [400/469] Loss D: 0.5869 Loss G: 1.3354\n",
      "Epoch [6/20] Lote [0/469] Loss D: 0.7621 Loss G: 1.3657\n",
      "Epoch [6/20] Lote [200/469] Loss D: 0.6089 Loss G: 1.7130\n",
      "Epoch [6/20] Lote [400/469] Loss D: 0.7736 Loss G: 2.1560\n",
      "Epoch [7/20] Lote [0/469] Loss D: 0.6908 Loss G: 1.5053\n",
      "Epoch [7/20] Lote [200/469] Loss D: 0.7900 Loss G: 1.6353\n",
      "Epoch [7/20] Lote [400/469] Loss D: 0.7141 Loss G: 1.2780\n",
      "Epoch [8/20] Lote [0/469] Loss D: 0.6352 Loss G: 1.4269\n",
      "Epoch [8/20] Lote [200/469] Loss D: 0.8178 Loss G: 1.8606\n",
      "Epoch [8/20] Lote [400/469] Loss D: 0.6065 Loss G: 1.9157\n",
      "Epoch [9/20] Lote [0/469] Loss D: 0.6938 Loss G: 1.2424\n",
      "Epoch [9/20] Lote [200/469] Loss D: 0.7548 Loss G: 1.8838\n",
      "Epoch [9/20] Lote [400/469] Loss D: 0.7697 Loss G: 1.1904\n",
      "Epoch [10/20] Lote [0/469] Loss D: 0.6823 Loss G: 0.9450\n",
      "Epoch [10/20] Lote [200/469] Loss D: 0.6674 Loss G: 1.6466\n",
      "Epoch [10/20] Lote [400/469] Loss D: 0.6349 Loss G: 2.0377\n",
      "Epoch [11/20] Lote [0/469] Loss D: 0.7614 Loss G: 1.2347\n",
      "Epoch [11/20] Lote [200/469] Loss D: 0.7305 Loss G: 1.2526\n",
      "Epoch [11/20] Lote [400/469] Loss D: 0.7047 Loss G: 0.9290\n",
      "Epoch [12/20] Lote [0/469] Loss D: 0.6907 Loss G: 1.8597\n",
      "Epoch [12/20] Lote [200/469] Loss D: 0.7326 Loss G: 1.6465\n",
      "Epoch [12/20] Lote [400/469] Loss D: 0.6907 Loss G: 1.6231\n",
      "Epoch [13/20] Lote [0/469] Loss D: 0.6395 Loss G: 1.7093\n",
      "Epoch [13/20] Lote [200/469] Loss D: 0.6658 Loss G: 2.3098\n",
      "Epoch [13/20] Lote [400/469] Loss D: 0.6267 Loss G: 1.6527\n",
      "Epoch [14/20] Lote [0/469] Loss D: 1.0758 Loss G: 0.8521\n",
      "Epoch [14/20] Lote [200/469] Loss D: 0.7637 Loss G: 2.3674\n",
      "Epoch [14/20] Lote [400/469] Loss D: 0.6822 Loss G: 2.4136\n",
      "Epoch [15/20] Lote [0/469] Loss D: 0.8581 Loss G: 0.9789\n",
      "Epoch [15/20] Lote [200/469] Loss D: 0.5681 Loss G: 1.7653\n",
      "Epoch [15/20] Lote [400/469] Loss D: 1.0032 Loss G: 2.8043\n",
      "Epoch [16/20] Lote [0/469] Loss D: 0.6919 Loss G: 2.4055\n",
      "Epoch [16/20] Lote [200/469] Loss D: 0.5327 Loss G: 1.7607\n",
      "Epoch [16/20] Lote [400/469] Loss D: 0.6830 Loss G: 1.5299\n",
      "Epoch [17/20] Lote [0/469] Loss D: 0.5921 Loss G: 1.7183\n",
      "Epoch [17/20] Lote [200/469] Loss D: 0.5336 Loss G: 2.2822\n",
      "Epoch [17/20] Lote [400/469] Loss D: 0.7274 Loss G: 1.1556\n",
      "Epoch [18/20] Lote [0/469] Loss D: 0.8065 Loss G: 3.0774\n",
      "Epoch [18/20] Lote [200/469] Loss D: 0.6974 Loss G: 1.6015\n",
      "Epoch [18/20] Lote [400/469] Loss D: 0.7086 Loss G: 1.6679\n",
      "Epoch [19/20] Lote [0/469] Loss D: 1.0864 Loss G: 1.1236\n",
      "Epoch [19/20] Lote [200/469] Loss D: 0.6219 Loss G: 2.5651\n",
      "Epoch [19/20] Lote [400/469] Loss D: 1.0310 Loss G: 0.6473\n",
      "Epoch [20/20] Lote [0/469] Loss D: 0.6601 Loss G: 1.3423\n",
      "Epoch [20/20] Lote [200/469] Loss D: 0.6760 Loss G: 1.4564\n",
      "Epoch [20/20] Lote [400/469] Loss D: 0.7235 Loss G: 1.5143\n",
      "Entrenamiento finalizado.\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# 5. Inicializaci√≥n de modelos, p√©rdidas y optimizadores\n",
    "# ===========================\n",
    "G = Generator(latent_dim).to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "\n",
    "# Funci√≥n para inicializar pesos como recomienda DCGAN\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1 or classname.find(\"Linear\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "G.apply(weights_init_normal)\n",
    "D.apply(weights_init_normal)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# 6. Bucle de entrenamiento\n",
    "# ===========================\n",
    "fixed_noise = torch.randn(64, latent_dim, device=device)  # para monitorear generaci√≥n a lo largo de los epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real_imgs, _) in enumerate(train_loader):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        batch_size_cur = real_imgs.size(0)\n",
    "\n",
    "        # -----------------------------\n",
    "        # a) Entrenamiento del Discriminador\n",
    "        # -----------------------------\n",
    "        D.zero_grad()\n",
    "\n",
    "        # Etiquetas para real y fake\n",
    "        real_labels = torch.ones(batch_size_cur, 1, device=device)\n",
    "        fake_labels = torch.zeros(batch_size_cur, 1, device=device)\n",
    "\n",
    "        # 1) P√©rdida con im√°genes reales\n",
    "        outputs_real = D(real_imgs)\n",
    "        loss_real = criterion(outputs_real, real_labels)\n",
    "\n",
    "        # 2) P√©rdida con im√°genes falsas (generadas)\n",
    "        z = torch.randn(batch_size_cur, latent_dim, device=device)\n",
    "        fake_imgs = G(z)\n",
    "\n",
    "        outputs_fake = D(fake_imgs.detach())  # detach para no propagar gradiente a G\n",
    "        loss_fake = criterion(outputs_fake, fake_labels)\n",
    "\n",
    "        # P√©rdida total del discriminador\n",
    "        loss_D = loss_real + loss_fake\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # b) Entrenamiento del Generador\n",
    "        # -----------------------------\n",
    "        G.zero_grad()\n",
    "\n",
    "        # Queremos que D(G(z)) ‚âà 1 (es decir, enga√±ar al discriminador)\n",
    "        outputs_fake_for_G = D(fake_imgs)\n",
    "        # NOTA: aqu√≠ usamos real_labels porque el generador quiere que sus im√°genes sean clasificadas como \"reales\"\n",
    "        loss_G = criterion(outputs_fake_for_G, real_labels)\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # Logging b√°sico\n",
    "        # -----------------------------\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "                  f\"Lote [{batch_idx}/{len(train_loader)}] \"\n",
    "                  f\"Loss D: {loss_D.item():.4f} \"\n",
    "                  f\"Loss G: {loss_G.item():.4f}\")\n",
    "\n",
    "    # ===========================\n",
    "    # 7. Guardar muestras por epoch\n",
    "    # ===========================\n",
    "    with torch.no_grad():\n",
    "        fake_samples = G(fixed_noise).detach().cpu()\n",
    "        # Des-normalizar de [-1,1] a [0,1] para guardar\n",
    "        fake_samples = (fake_samples + 1) / 2.0\n",
    "        utils.save_image(\n",
    "            fake_samples,\n",
    "            f\"samples/fake_epoch_{epoch+1:03d}.png\",\n",
    "            nrow=8\n",
    "        )\n",
    "\n",
    "    # Opcional: guardar pesos\n",
    "    torch.save(G.state_dict(), \"G_mnist.pth\")\n",
    "    torch.save(D.state_dict(), \"D_mnist.pth\")\n",
    "\n",
    "print(\"Entrenamiento finalizado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f5b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(G.state_dict(), \"G_mnist.pth\")\n",
    "# latent_dim = 100\n",
    "# G = Generator(latent_dim).to(device)\n",
    "# G.load_state_dict(torch.load(\"G_mnist.pth\", map_location=device))\n",
    "# G.eval()   # modo inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f84da0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de im√°genes a generar\n",
    "n_samples = 64\n",
    "\n",
    "# Vector de ruido aleatorio\n",
    "z = torch.randn(n_samples, latent_dim).to(device)\n",
    "\n",
    "# Generaci√≥n\n",
    "with torch.no_grad():\n",
    "    fake_images = G(z)\n",
    "\n",
    "# Las im√°genes est√°n en [-1, 1] por el Tanh ‚Üí las llevamos a [0,1]\n",
    "fake_images = (fake_images + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a38dcaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = fake_images[9].cpu().squeeze()   # quitar dimensiones extra\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
