{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d29a4f51",
   "metadata": {},
   "source": [
    "# ðŸ«€ ECG Synthetic Signal Generation using Hybrid CNN-LSTM GAN\n",
    "\n",
    "## Overview\n",
    "This notebook implements an advanced **Wasserstein GAN with Gradient Penalty (WGAN-GP)** that combines:\n",
    "- **1D Convolutional Neural Networks (CNN)** for hierarchical feature extraction\n",
    "- **Long Short-Term Memory networks (LSTM)** for capturing temporal dependencies\n",
    "- **Attention mechanisms** (optional) for focusing on important ECG patterns\n",
    "\n",
    "## Architecture Highlights\n",
    "- **Generator**: Noise â†’ FC â†’ CNN Upsampling â†’ Bidirectional LSTM â†’ CNN Refinement â†’ ECG Signal\n",
    "- **Discriminator**: ECG Signal â†’ CNN Features â†’ Bidirectional LSTM â†’ Attention/Classification â†’ Real/Fake\n",
    "- **Training**: WGAN-GP with gradient penalty for stable training and high-quality signal generation\n",
    "\n",
    "## Key Features\n",
    "âœ… Temporal coherence through LSTM layers  \n",
    "âœ… Multi-scale feature extraction via CNN  \n",
    "âœ… Stable training with Wasserstein loss  \n",
    "âœ… Comprehensive ECG-specific evaluation metrics  \n",
    "âœ… R-peak detection and heart rate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5cbae41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NSR': array([[-0.06451476, -0.05951476, -0.02951476, ...,  0.13548524,\n",
       "          0.13048524,  0.11548524],\n",
       "        [ 0.24548524,  0.25048524,  0.24048524, ...,  0.34048524,\n",
       "          0.33548524,  0.31548524],\n",
       "        [ 0.13048524,  0.12548524,  0.11048524, ..., -0.20451476,\n",
       "         -0.20451476, -0.20451476],\n",
       "        ...,\n",
       "        [-0.46451476, -0.46451476, -0.46451476, ..., -0.59451476,\n",
       "         -0.58951476, -0.58951476],\n",
       "        [-0.53451476, -0.51451476, -0.53451476, ..., -0.51451476,\n",
       "         -0.44451476, -0.40451476],\n",
       "        [ 0.51548524,  0.50548524,  0.50548524, ...,  0.21048524,\n",
       "          0.20048524,  0.18548524]]),\n",
       " 'VT': array([[ 0.10787069,  0.08787069,  0.08787069, ...,  0.01287069,\n",
       "         -0.00212931,  0.01787069],\n",
       "        [-0.31212931, -0.29712931, -0.27212931, ...,  0.82787069,\n",
       "          0.84787069,  0.86787069],\n",
       "        [ 0.01287069,  0.00787069,  0.00287069, ...,  0.04787069,\n",
       "          0.06787069,  0.10287069],\n",
       "        ...,\n",
       "        [ 0.17287069,  0.16787069,  0.16287069, ..., -0.08212931,\n",
       "         -0.06712931, -0.07212931],\n",
       "        [-0.23712931, -0.22712931, -0.21712931, ...,  1.69287069,\n",
       "          1.68787069,  1.63287069],\n",
       "        [ 0.14787069,  0.17787069,  0.20787069, ..., -0.01212931,\n",
       "         -0.01212931, -0.01212931]]),\n",
       " 'IVR': array([[ 0.05063069,  0.04563069,  0.04063069, ..., -0.10936931,\n",
       "         -0.11436931, -0.11436931],\n",
       "        [ 0.19063069,  0.20063069,  0.19563069, ..., -0.02436931,\n",
       "         -0.02436931, -0.00936931],\n",
       "        [ 0.02063069,  0.02563069,  0.03063069, ...,  0.23563069,\n",
       "          0.26563069,  0.28063069],\n",
       "        ...,\n",
       "        [-0.02436931, -0.01936931, -0.03436931, ...,  0.07063069,\n",
       "          0.08563069,  0.07063069],\n",
       "        [ 0.73563069,  0.74563069,  0.68063069, ..., -0.08936931,\n",
       "         -0.08936931, -0.09436931],\n",
       "        [ 0.11063069,  0.12063069,  0.12063069, ...,  0.35563069,\n",
       "          0.39063069,  0.45063069]]),\n",
       " 'VFL': array([[-0.05835083, -0.01835083,  0.00664917, ...,  0.12164917,\n",
       "          0.12164917,  0.14664917],\n",
       "        [-0.14335083, -0.13335083, -0.12335083, ..., -0.03335083,\n",
       "         -0.02335083,  0.01164917],\n",
       "        [ 0.01664917,  0.02664917,  0.03164917, ..., -0.16335083,\n",
       "         -0.16835083, -0.15335083],\n",
       "        ...,\n",
       "        [-0.04835083, -0.04835083, -0.03335083, ...,  0.08164917,\n",
       "          0.09664917,  0.12664917],\n",
       "        [ 0.39164917,  0.41664917,  0.44664917, ...,  0.06664917,\n",
       "          0.04164917,  0.03164917],\n",
       "        [ 0.03664917,  0.07664917,  0.12664917, ...,  0.17664917,\n",
       "          0.16664917,  0.17164917]]),\n",
       " 'Fusion': array([[ 0.24582967,  0.26082967,  0.25082967, ...,  0.18082967,\n",
       "          0.17582967,  0.19082967],\n",
       "        [-1.14417033, -1.15417033, -1.17417033, ...,  0.31082967,\n",
       "          0.31082967,  0.30582967],\n",
       "        [ 0.20582967,  0.19082967,  0.19582967, ...,  0.18082967,\n",
       "          0.17082967,  0.14582967],\n",
       "        ...,\n",
       "        [-0.23917033, -0.23917033, -0.22917033, ..., -0.20917033,\n",
       "         -0.21917033, -0.22917033],\n",
       "        [ 0.15082967,  0.09082967,  0.04582967, ..., -0.57417033,\n",
       "         -0.56417033, -0.57417033],\n",
       "        [-0.59417033, -0.55417033, -0.50917033, ...,  1.81582967,\n",
       "          1.91082967,  2.01582967]]),\n",
       " 'LBBBB': array([[-0.09205654, -0.09705654, -0.09705654, ..., -0.00705654,\n",
       "         -0.00205654, -0.00705654],\n",
       "        [-0.03205654, -0.03205654, -0.00705654, ...,  0.03294346,\n",
       "          0.03794346,  0.04294346],\n",
       "        [-0.00205654, -0.00205654, -0.01705654, ..., -0.19205654,\n",
       "         -0.19705654, -0.17705654],\n",
       "        ...,\n",
       "        [-0.48705654, -0.52705654, -0.55705654, ..., -0.30705654,\n",
       "         -0.41705654, -0.51705654],\n",
       "        [-0.31705654, -0.33705654, -0.33705654, ...,  0.12794346,\n",
       "          0.15794346,  0.15794346],\n",
       "        [-0.02205654, -0.01205654, -0.00705654, ..., -0.10205654,\n",
       "         -0.07705654, -0.05705654]]),\n",
       " 'RBBBB': array([[ 0.66243907,  0.61743907,  0.57743907, ...,  1.21243907,\n",
       "          0.76743907,  0.40243907],\n",
       "        [-0.47756093, -0.44256093, -0.44256093, ..., -0.56256093,\n",
       "         -0.57256093, -0.56756093],\n",
       "        [ 1.89243907,  1.81243907,  1.69243907, ..., -0.25756093,\n",
       "         -0.25756093, -0.26256093],\n",
       "        ...,\n",
       "        [-0.08756093, -0.08756093, -0.09756093, ..., -0.30756093,\n",
       "         -0.30756093, -0.29756093],\n",
       "        [ 1.33743907,  0.85243907,  0.36743907, ...,  0.47743907,\n",
       "          0.48743907,  0.50243907],\n",
       "        [ 0.38243907,  0.37243907,  0.39243907, ...,  0.35243907,\n",
       "          0.38243907,  0.40743907]]),\n",
       " 'SDHB': array([[-0.1526725, -0.1526725, -0.1426725, ..., -0.0726725, -0.0776725,\n",
       "         -0.0676725],\n",
       "        [ 0.0623275,  0.0773275,  0.0523275, ..., -0.1826725, -0.1776725,\n",
       "         -0.1676725],\n",
       "        [-0.0726725, -0.0676725, -0.0576725, ..., -0.1076725, -0.1226725,\n",
       "         -0.1276725],\n",
       "        ...,\n",
       "        [-0.0126725, -0.0176725, -0.0226725, ..., -0.0426725, -0.0226725,\n",
       "         -0.0076725],\n",
       "        [ 0.0023275,  0.0023275,  0.0073275, ...,  0.1673275,  0.1723275,\n",
       "          0.1823275],\n",
       "        [-0.0826725, -0.0926725, -0.0926725, ..., -0.0976725, -0.1026725,\n",
       "         -0.1176725]]),\n",
       " 'PR': array([[ 0.4307984,  0.3857984,  0.3457984, ...,  0.8807984,  0.8957984,\n",
       "          0.9307984],\n",
       "        [ 0.3407984,  0.3557984,  0.3607984, ...,  1.3157984,  1.5207984,\n",
       "          1.4657984],\n",
       "        [-0.0842016, -0.0942016, -0.1042016, ..., -0.1142016, -0.0842016,\n",
       "         -0.0692016],\n",
       "        ...,\n",
       "        [ 0.3957984,  0.3707984,  0.3457984, ..., -0.5142016, -0.4242016,\n",
       "         -0.3242016],\n",
       "        [ 0.2857984,  0.3107984,  0.3107984, ..., -1.6792016, -1.7642016,\n",
       "         -1.8242016],\n",
       "        [ 0.9507984,  0.9907984,  1.0107984, ..., -1.5492016, -1.4492016,\n",
       "         -1.3642016]]),\n",
       " 'APB': array([[-0.00630398, -0.02630398, -0.01630398, ..., -0.25130398,\n",
       "         -0.25630398, -0.25630398],\n",
       "        [-0.21130398, -0.20630398, -0.21630398, ...,  0.29369602,\n",
       "          0.52369602,  0.77369602],\n",
       "        [-0.12630398, -0.13130398, -0.14630398, ..., -0.22630398,\n",
       "         -0.23630398, -0.23630398],\n",
       "        ...,\n",
       "        [ 0.17369602,  0.16369602,  0.15869602, ...,  0.03869602,\n",
       "          0.01869602, -0.00130398],\n",
       "        [-0.08130398, -0.09630398, -0.10130398, ..., -0.15630398,\n",
       "         -0.15630398, -0.16130398],\n",
       "        [-0.04630398, -0.05630398, -0.06630398, ..., -0.11630398,\n",
       "         -0.12130398, -0.12130398]]),\n",
       " 'AFL': array([[-0.02385639, -0.00885639,  0.01614361, ...,  0.02614361,\n",
       "          0.01614361,  0.00114361],\n",
       "        [ 0.12614361,  0.09614361,  0.11114361, ...,  0.76614361,\n",
       "          0.84114361,  0.85114361],\n",
       "        [-0.06885639, -0.09385639, -0.08885639, ..., -0.17385639,\n",
       "         -0.17885639, -0.17385639],\n",
       "        ...,\n",
       "        [-0.20385639, -0.23885639, -0.24385639, ...,  0.05114361,\n",
       "          0.05614361,  0.04114361],\n",
       "        [-0.19885639, -0.19885639, -0.19385639, ..., -0.26885639,\n",
       "         -0.26885639, -0.28885639],\n",
       "        [-0.00385639,  0.01114361,  0.01614361, ..., -0.18385639,\n",
       "         -0.18885639, -0.18385639]]),\n",
       " 'AFIB': array([[ 0.45437483,  0.57937483,  0.71937483, ..., -0.10562517,\n",
       "         -0.09062517, -0.09062517],\n",
       "        [ 0.91937483,  0.59437483,  0.34937483, ..., -0.12562517,\n",
       "         -0.11062517, -0.10562517],\n",
       "        [-0.08562517, -0.09062517, -0.07562517, ..., -0.08062517,\n",
       "         -0.07062517, -0.06062517],\n",
       "        ...,\n",
       "        [-0.22562517, -0.21062517, -0.19562517, ..., -0.07562517,\n",
       "         -0.09062517, -0.12062517],\n",
       "        [-0.02562517, -0.04562517, -0.04562517, ..., -0.13062517,\n",
       "         -0.11562517, -0.11562517],\n",
       "        [ 0.30937483,  0.31437483,  0.31437483, ...,  0.12437483,\n",
       "          0.12437483,  0.13937483]]),\n",
       " 'SVTA': array([[-0.09665128, -0.11165128, -0.14165128, ..., -0.12165128,\n",
       "         -0.12665128, -0.11665128],\n",
       "        [-0.13665128, -0.10665128, -0.08665128, ...,  0.00334872,\n",
       "          0.00834872,  0.00334872],\n",
       "        [-0.13165128, -0.07665128, -0.05665128, ..., -0.14165128,\n",
       "         -0.12165128, -0.12165128],\n",
       "        ...,\n",
       "        [-0.86665128, -0.86665128, -0.81665128, ...,  0.21834872,\n",
       "          0.23334872,  0.26334872],\n",
       "        [ 0.09334872,  0.11834872,  0.15334872, ...,  0.08334872,\n",
       "          0.08834872,  0.07334872],\n",
       "        [ 0.32334872,  0.29834872,  0.29834872, ..., -0.07165128,\n",
       "         -0.08165128, -0.08665128]]),\n",
       " 'WPW': array([[ 0.03522831,  0.04022831,  0.05022831, ...,  0.13022831,\n",
       "          0.13522831,  0.12522831],\n",
       "        [-0.20477169, -0.21977169, -0.22977169, ...,  1.58022831,\n",
       "          1.62022831,  1.58022831],\n",
       "        [ 1.46522831,  1.27522831,  1.00522831, ...,  0.35022831,\n",
       "          0.32022831,  0.30522831],\n",
       "        ...,\n",
       "        [-0.13477169, -0.12977169, -0.10977169, ...,  0.00522831,\n",
       "         -0.00477169, -0.00477169],\n",
       "        [-0.12977169, -0.12977169, -0.13977169, ..., -0.10477169,\n",
       "         -0.09477169, -0.08477169],\n",
       "        [ 0.07022831,  0.04022831,  0.03522831, ..., -0.14477169,\n",
       "         -0.13477169, -0.11477169]]),\n",
       " 'PVC': array([[ 0.14564412,  0.14564412,  0.14064412, ...,  0.41064412,\n",
       "          0.43064412,  0.46064412],\n",
       "        [-1.06435588, -1.06935588, -1.05435588, ..., -1.09435588,\n",
       "         -1.08935588, -1.09935588],\n",
       "        [ 0.10564412,  0.09064412,  0.07064412, ...,  0.16064412,\n",
       "          0.16564412,  0.17064412],\n",
       "        ...,\n",
       "        [-0.06435588, -0.05435588, -0.05435588, ...,  0.26564412,\n",
       "          0.28064412,  0.29064412],\n",
       "        [-0.08435588, -0.08435588, -0.08435588, ..., -0.17935588,\n",
       "         -0.19935588, -0.18435588],\n",
       "        [ 0.22564412,  0.23564412,  0.22564412, ..., -0.71435588,\n",
       "         -0.70435588, -0.68935588]]),\n",
       " 'Bigeminy': array([[-0.11918164, -0.11918164, -0.11918164, ..., -0.28918164,\n",
       "         -0.28418164, -0.26918164],\n",
       "        [-0.27918164, -0.28418164, -0.26418164, ...,  0.88581836,\n",
       "          1.01081836,  1.12581836],\n",
       "        [ 0.61581836,  0.47081836,  0.37581836, ...,  0.33081836,\n",
       "          0.35581836,  0.38581836],\n",
       "        ...,\n",
       "        [ 0.62081836,  0.62081836,  0.62081836, ...,  0.18081836,\n",
       "          0.16081836,  0.14081836],\n",
       "        [-0.43918164, -0.44418164, -0.44918164, ..., -0.37918164,\n",
       "         -0.37418164, -0.37918164],\n",
       "        [-0.57918164, -0.53418164, -0.47918164, ..., -0.13918164,\n",
       "         -0.11418164, -0.09418164]]),\n",
       " 'Trigeminy': array([[ 0.21956848,  0.22456848,  0.21956848, ...,  0.19956848,\n",
       "          0.18956848,  0.19456848],\n",
       "        [-0.40543152, -0.40043152, -0.40543152, ..., -0.54543152,\n",
       "         -0.54543152, -0.54543152],\n",
       "        [ 0.10456848,  0.09456848,  0.08456848, ...,  0.06956848,\n",
       "          0.08456848,  0.08956848],\n",
       "        ...,\n",
       "        [ 0.05456848,  0.05456848,  0.03956848, ...,  0.11956848,\n",
       "          0.12956848,  0.12956848],\n",
       "        [ 0.08956848,  0.07956848,  0.06956848, ...,  0.08956848,\n",
       "          0.08956848,  0.08456848],\n",
       "        [ 0.21956848,  0.21456848,  0.23456848, ...,  0.06956848,\n",
       "          0.07456848,  0.06956848]])}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "path_db = \"../../ECG_DATASET/dataset_ekg.pkl\"\n",
    "\n",
    "with open(path_db, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "440eabe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dataset = dataset['NSR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b586ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_dataset.shape[1]\n",
    "\n",
    "fs = 360\n",
    "ts = 1/fs\n",
    "t = np.arange(N)*ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a405f8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original range: [-2.270, 3.045]\n",
      "Normalized range: [-1.000, 1.000]\n"
     ]
    }
   ],
   "source": [
    "# Normalizar al rango [-1, 1] para coincidir con Tanh\n",
    "X_min = X_dataset.min()\n",
    "X_max = X_dataset.max()\n",
    "X_dataset_normalized = 2 * (X_dataset - X_min) / (X_max - X_min) - 1\n",
    "\n",
    "print(f\"Original range: [{X_min:.3f}, {X_max:.3f}]\")\n",
    "print(f\"Normalized range: [{X_dataset_normalized.min():.3f}, {X_dataset_normalized.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e4d69d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 2025\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "class SineWaveDataset:\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "\n",
    "def get_dataloader(data, batch_size=32, shuffle=True):\n",
    "    dataset = SineWaveDataset(data)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce40fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== HYBRID CNN-LSTM GAN ARCHITECTURE FOR ECG SIGNALS ==========\n",
    "# Combining CNNs for feature extraction and LSTMs for temporal dependencies\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"Weight initialization for convolutional and linear layers\"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif classname.find('LSTM') != -1:\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "\n",
    "# ========== GENERATOR WITH CNN + LSTM ==========\n",
    "class CNNLSTMGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid Generator: CNN for upsampling + LSTM for temporal coherence\n",
    "    \"\"\"\n",
    "    def __init__(self, noise_dim=100, output_length=3600, lstm_hidden=256, lstm_layers=2):\n",
    "        super().__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.output_length = output_length\n",
    "        self.lstm_hidden = lstm_hidden\n",
    "        \n",
    "        # Initial projection: 100 -> 128 * 225\n",
    "        self.init_size = output_length // 16  # 225 for 3600\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(noise_dim, 128 * self.init_size),\n",
    "            nn.BatchNorm1d(128 * self.init_size),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # CNN upsampling layers: 225 -> 900\n",
    "        self.conv_upsample = nn.Sequential(\n",
    "            # 225 -> 450\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 450 -> 900\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        # LSTM for temporal dependencies (operates on 900 timesteps)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=32,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3 if lstm_layers > 1 else 0,\n",
    "            bidirectional=True  # Bidirectional for better context\n",
    "        )\n",
    "        \n",
    "        # Post-LSTM CNN refinement: 900 -> 3600\n",
    "        self.post_lstm_conv = nn.Sequential(\n",
    "            nn.Conv1d(lstm_hidden * 2, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 900 -> 1800\n",
    "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 1800 -> 3600\n",
    "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # Final layer\n",
    "            nn.Conv1d(32, 1, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # Project noise\n",
    "        x = self.fc(z)\n",
    "        x = x.view(-1, 128, self.init_size)\n",
    "        \n",
    "        # CNN upsampling\n",
    "        x = self.conv_upsample(x)  # [batch, 32, 900]\n",
    "        \n",
    "        # Prepare for LSTM: [batch, seq_len, features]\n",
    "        x = x.permute(0, 2, 1)  # [batch, 900, 32]\n",
    "        \n",
    "        # LSTM processing for temporal coherence\n",
    "        x, _ = self.lstm(x)  # [batch, 900, lstm_hidden*2]\n",
    "        \n",
    "        # Back to CNN format: [batch, features, seq_len]\n",
    "        x = x.permute(0, 2, 1)  # [batch, lstm_hidden*2, 900]\n",
    "        \n",
    "        # Final upsampling and refinement\n",
    "        x = self.post_lstm_conv(x)  # [batch, 1, 3600]\n",
    "        \n",
    "        return x.squeeze(1)\n",
    "\n",
    "# ========== DISCRIMINATOR WITH CNN + LSTM ==========\n",
    "class CNNLSTMDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Hybrid Discriminator: CNN for feature extraction + LSTM for temporal analysis\n",
    "    \"\"\"\n",
    "    def __init__(self, input_length=3600, lstm_hidden=256, lstm_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN feature extraction: 3600 -> 900\n",
    "        self.conv_features = nn.Sequential(\n",
    "            # 3600 -> 1800\n",
    "            nn.Conv1d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # 1800 -> 900\n",
    "            nn.Conv1d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # 900 -> 900 (refinement)\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        \n",
    "        # LSTM for temporal pattern recognition\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3 if lstm_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Post-LSTM processing\n",
    "        self.post_lstm_conv = nn.Sequential(\n",
    "            nn.Conv1d(lstm_hidden * 2, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            # 900 -> 450\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        \n",
    "        # Final classification\n",
    "        flattened_size = 512 * 450\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(flattened_size, 512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Add channel dimension if needed\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        x = self.conv_features(x)  # [batch, 128, 900]\n",
    "        \n",
    "        # Prepare for LSTM\n",
    "        x = x.permute(0, 2, 1)  # [batch, 900, 128]\n",
    "        \n",
    "        # LSTM temporal analysis\n",
    "        x, _ = self.lstm(x)  # [batch, 900, lstm_hidden*2]\n",
    "        \n",
    "        # Back to CNN format\n",
    "        x = x.permute(0, 2, 1)  # [batch, lstm_hidden*2, 900]\n",
    "        \n",
    "        # Post-LSTM convolutions\n",
    "        x = self.post_lstm_conv(x)  # [batch, 512, 450]\n",
    "        \n",
    "        # Flatten and classify\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# ========== ATTENTION-ENHANCED LSTM DISCRIMINATOR (Alternative) ==========\n",
    "class AttentionLSTMDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator with Self-Attention mechanism for focusing on important ECG features\n",
    "    \"\"\"\n",
    "    def __init__(self, input_length=3600, lstm_hidden=256, lstm_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial CNN: 3600 -> 900\n",
    "        self.conv_features = nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "        )\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3 if lstm_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Self-Attention mechanism\n",
    "        self.attention_dim = lstm_hidden * 2\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.attention_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Final classification layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.attention_dim, 256),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        x = self.conv_features(x)  # [batch, 128, 900]\n",
    "        x = x.permute(0, 2, 1)  # [batch, 900, 128]\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(x)  # [batch, 900, lstm_hidden*2]\n",
    "        \n",
    "        # Self-Attention\n",
    "        attention_weights = self.attention(lstm_out)  # [batch, 900, 1]\n",
    "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
    "        \n",
    "        # Weighted sum of LSTM outputs\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)  # [batch, lstm_hidden*2]\n",
    "        \n",
    "        # Classification\n",
    "        out = self.fc(context)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a687c11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Training samples: 283\n",
      "Signal length: 3600\n",
      "Batch size: 32\n",
      "LSTM hidden units: 256, layers: 2\n",
      "\n",
      "==================================================\n",
      "Using CNN_LSTM Architecture\n",
      "==================================================\n",
      "Using CNN-LSTM Discriminator\n",
      "\n",
      "Generator parameters: 6,223,393\n",
      "Discriminator parameters: 121,286,625\n",
      "Using CNN-LSTM Discriminator\n",
      "\n",
      "Generator parameters: 6,223,393\n",
      "Discriminator parameters: 121,286,625\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 148\u001b[39m\n\u001b[32m    145\u001b[39m d_optimizer = Adam(discriminator.parameters(), lr=\u001b[32m0.0001\u001b[39m, betas=(\u001b[32m0.5\u001b[39m, \u001b[32m0.999\u001b[39m))\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# Learning rate schedulers\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m g_scheduler = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mg_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmin\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    150\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m d_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n\u001b[32m    152\u001b[39m     d_optimizer, mode=\u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m, factor=\u001b[32m0.5\u001b[39m, patience=\u001b[32m100\u001b[39m, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    153\u001b[39m )\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# ========== TRAINER SETUP ==========\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "# ========== WGAN-GP TRAINER WITH IMPROVED LOSS ==========\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, device):\n",
    "    \"\"\"Calculates the gradient penalty for WGAN-GP\"\"\"\n",
    "    batch_size = real_samples.size(0)\n",
    "    alpha = torch.rand(batch_size, 1).to(device)\n",
    "    \n",
    "    # Interpolated samples\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    \n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    \n",
    "    fake = torch.ones(batch_size, 1).to(device)\n",
    "    \n",
    "    # Get gradients\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "class WGANGPTrainer:\n",
    "    \"\"\"Enhanced Wasserstein GAN with Gradient Penalty trainer\"\"\"\n",
    "    def __init__(self, generator, discriminator, g_optimizer, d_optimizer, device, \n",
    "                 lambda_gp=10, n_critic=5):\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.device = device\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.n_critic = n_critic\n",
    "        self.d_steps = 0\n",
    "\n",
    "    def train_step(self, real_data):\n",
    "        batch_size = real_data.size(0)\n",
    "        real_data = real_data.to(self.device)\n",
    "\n",
    "        # ===== Train Discriminator =====\n",
    "        self.d_optimizer.zero_grad()\n",
    "\n",
    "        # Real samples\n",
    "        d_real = self.discriminator(real_data).mean()\n",
    "\n",
    "        # Fake samples\n",
    "        noise = torch.randn(batch_size, noise_dim).to(self.device)\n",
    "        fake_data = self.generator(noise)\n",
    "        d_fake = self.discriminator(fake_data.detach()).mean()\n",
    "\n",
    "        # Gradient penalty\n",
    "        gp = compute_gradient_penalty(self.discriminator, real_data, fake_data, self.device)\n",
    "\n",
    "        # Wasserstein loss with gradient penalty\n",
    "        d_loss = d_fake - d_real + self.lambda_gp * gp\n",
    "        d_loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), max_norm=1.0)\n",
    "        self.d_optimizer.step()\n",
    "\n",
    "        self.d_steps += 1\n",
    "\n",
    "        # ===== Train Generator (every n_critic steps) =====\n",
    "        g_loss = torch.tensor(0.0)\n",
    "        if self.d_steps % self.n_critic == 0:\n",
    "            self.g_optimizer.zero_grad()\n",
    "            noise = torch.randn(batch_size, noise_dim).to(self.device)\n",
    "            fake_data = self.generator(noise)\n",
    "            g_loss = -self.discriminator(fake_data).mean()\n",
    "            g_loss.backward()\n",
    "            \n",
    "            # Gradient clipping for generator\n",
    "            torch.nn.utils.clip_grad_norm_(self.generator.parameters(), max_norm=1.0)\n",
    "            self.g_optimizer.step()\n",
    "\n",
    "        # Return metrics\n",
    "        wasserstein_d = (d_real - d_fake).item()\n",
    "        return d_loss.item(), g_loss.item(), wasserstein_d, gp.item()\n",
    "\n",
    "# ========== HYPERPARAMETERS ==========\n",
    "noise_dim = 128  # Increased for more variety\n",
    "batch_size = 32  # Reduced for LSTM memory efficiency\n",
    "lstm_hidden = 256\n",
    "lstm_layers = 2\n",
    "output_dim = X_dataset_normalized.shape[1]\n",
    "\n",
    "# ========== MODEL INITIALIZATION ==========\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Training samples: {X_dataset_normalized.shape[0]}\")\n",
    "print(f\"Signal length: {output_dim}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"LSTM hidden units: {lstm_hidden}, layers: {lstm_layers}\")\n",
    "\n",
    "# Choose architecture: 'cnn_lstm' or 'attention_lstm'\n",
    "architecture = 'cnn_lstm'  # Options: 'cnn_lstm', 'attention_lstm'\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Using {architecture.upper()} Architecture\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize Generator (always CNN-LSTM hybrid)\n",
    "generator = CNNLSTMGenerator(\n",
    "    noise_dim=noise_dim, \n",
    "    output_length=output_dim,\n",
    "    lstm_hidden=lstm_hidden,\n",
    "    lstm_layers=lstm_layers\n",
    ").to(device)\n",
    "\n",
    "# Initialize Discriminator\n",
    "if architecture == 'attention_lstm':\n",
    "    discriminator = AttentionLSTMDiscriminator(\n",
    "        input_length=output_dim,\n",
    "        lstm_hidden=lstm_hidden,\n",
    "        lstm_layers=lstm_layers\n",
    "    ).to(device)\n",
    "    print(\"Using Attention-Enhanced LSTM Discriminator\")\n",
    "else:\n",
    "    discriminator = CNNLSTMDiscriminator(\n",
    "        input_length=output_dim,\n",
    "        lstm_hidden=lstm_hidden,\n",
    "        lstm_layers=lstm_layers\n",
    "    ).to(device)\n",
    "    print(\"Using CNN-LSTM Discriminator\")\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nGenerator parameters: {count_parameters(generator):,}\")\n",
    "print(f\"Discriminator parameters: {count_parameters(discriminator):,}\")\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init)\n",
    "discriminator.apply(weights_init)\n",
    "\n",
    "# ========== OPTIMIZERS WITH SCHEDULER ==========\n",
    "# Lower learning rates for LSTM stability\n",
    "g_optimizer = Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "d_optimizer = Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "\n",
    "# Learning rate schedulers\n",
    "g_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    g_optimizer, mode='min', factor=0.5, patience=100, verbose=True\n",
    ")\n",
    "d_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    d_optimizer, mode='min', factor=0.5, patience=100, verbose=True\n",
    ")\n",
    "\n",
    "# ========== TRAINER SETUP ==========\n",
    "trainer = WGANGPTrainer(\n",
    "    generator, discriminator, \n",
    "    g_optimizer, d_optimizer, \n",
    "    device, \n",
    "    lambda_gp=10,\n",
    "    n_critic=5\n",
    ")\n",
    "\n",
    "# ========== TRAINING LOOP ==========\n",
    "num_epochs = 3000  # More epochs for LSTM convergence\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "w_distances = []\n",
    "gp_values = []\n",
    "\n",
    "EPOCHS_TO_PLOT = 100  # Plot more frequently\n",
    "SAVE_CHECKPOINT_EVERY = 500\n",
    "\n",
    "# Use normalized data\n",
    "dataloader_normalized = get_dataloader(X_dataset_normalized, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting CNN-LSTM WGAN-GP Training\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "best_w_distance = float('-inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_d_loss = []\n",
    "    epoch_g_loss = []\n",
    "    epoch_wd = []\n",
    "    epoch_gp = []\n",
    "    \n",
    "    for real_data in dataloader_normalized:\n",
    "        d_loss, g_loss, w_d, gp = trainer.train_step(real_data)\n",
    "        epoch_d_loss.append(d_loss)\n",
    "        epoch_g_loss.append(g_loss if isinstance(g_loss, float) else g_loss.item())\n",
    "        epoch_wd.append(w_d)\n",
    "        epoch_gp.append(gp)\n",
    "    \n",
    "    # Store average losses per epoch\n",
    "    avg_d_loss = np.mean(epoch_d_loss)\n",
    "    avg_g_loss = np.mean(epoch_g_loss)\n",
    "    avg_wd = np.mean(epoch_wd)\n",
    "    avg_gp = np.mean(epoch_gp)\n",
    "    \n",
    "    d_losses.append(avg_d_loss)\n",
    "    g_losses.append(avg_g_loss)\n",
    "    w_distances.append(avg_wd)\n",
    "    gp_values.append(avg_gp)\n",
    "    \n",
    "    # Update learning rate schedulers\n",
    "    g_scheduler.step(avg_g_loss)\n",
    "    d_scheduler.step(avg_d_loss)\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch+1) % 50 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'  D Loss: {avg_d_loss:.4f} | G Loss: {avg_g_loss:.4f}')\n",
    "        print(f'  Wasserstein Distance: {avg_wd:.4f} | GP: {avg_gp:.4f}')\n",
    "        print(f'  LR - G: {g_optimizer.param_groups[0][\"lr\"]:.6f} | D: {d_optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_wd > best_w_distance:\n",
    "        best_w_distance = avg_wd\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator': generator.state_dict(),\n",
    "            'discriminator': discriminator.state_dict(),\n",
    "            'wasserstein_distance': best_w_distance\n",
    "        }, 'best_ecg_lstm_gan.pth')\n",
    "\n",
    "    # Plot progress\n",
    "    if (epoch+1) % EPOCHS_TO_PLOT == 0:\n",
    "        plt.figure(figsize=(18, 12))\n",
    "        generator.eval()\n",
    "        with torch.no_grad():\n",
    "            # Generate multiple samples\n",
    "            noise = torch.randn(4, noise_dim).to(device)\n",
    "            generated_samples = generator(noise).cpu().numpy()\n",
    "            # Denormalize\n",
    "            generated_samples_denorm = (generated_samples + 1) / 2 * (X_max - X_min) + X_min\n",
    "        generator.train()\n",
    "\n",
    "        # Plot 4 generated samples\n",
    "        for i in range(4):\n",
    "            plt.subplot(4, 3, i+1)\n",
    "            plt.plot(t, generated_samples_denorm[i], color='blue', alpha=0.8, linewidth=0.6)\n",
    "            plt.xlabel('Time [s]', fontsize=9)\n",
    "            plt.ylabel('Amplitude', fontsize=9)\n",
    "            plt.title(f'Generated Sample {i+1} - Epoch {epoch+1}', fontsize=10)\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.xlim([0, 10])\n",
    "        \n",
    "        # Plot real samples for comparison\n",
    "        for i in range(2):\n",
    "            plt.subplot(4, 3, 5+i)\n",
    "            idx = np.random.randint(0, X_dataset.shape[0])\n",
    "            plt.plot(t, X_dataset[idx], color='green', alpha=0.8, linewidth=0.6)\n",
    "            plt.xlabel('Time [s]', fontsize=9)\n",
    "            plt.ylabel('Amplitude', fontsize=9)\n",
    "            plt.title(f'Real ECG Sample {i+1}', fontsize=10)\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.xlim([0, 10])\n",
    "\n",
    "        # Plot training metrics\n",
    "        plt.subplot(4, 3, 7)\n",
    "        plt.plot(d_losses, label='D Loss', alpha=0.7, linewidth=1)\n",
    "        plt.plot(g_losses, label='G Loss', alpha=0.7, linewidth=1)\n",
    "        plt.xlabel('Epoch', fontsize=9)\n",
    "        plt.ylabel('Loss', fontsize=9)\n",
    "        plt.title('Training Losses', fontsize=10)\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        plt.subplot(4, 3, 8)\n",
    "        plt.plot(w_distances, color='purple', alpha=0.7, linewidth=1)\n",
    "        plt.xlabel('Epoch', fontsize=9)\n",
    "        plt.ylabel('W-Distance', fontsize=9)\n",
    "        plt.title(f'Wasserstein Distance (Best: {best_w_distance:.3f})', fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        plt.subplot(4, 3, 9)\n",
    "        plt.plot(gp_values, color='red', alpha=0.7, linewidth=1)\n",
    "        plt.xlabel('Epoch', fontsize=9)\n",
    "        plt.ylabel('Gradient Penalty', fontsize=9)\n",
    "        plt.title('Gradient Penalty', fontsize=10)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Zoom into ECG segment (2 seconds)\n",
    "        plt.subplot(4, 3, 10)\n",
    "        segment_samples = int(2 * fs)  # 2 seconds\n",
    "        t_segment = t[:segment_samples]\n",
    "        plt.plot(t_segment, generated_samples_denorm[0][:segment_samples], \n",
    "                color='blue', alpha=0.8, linewidth=1, label='Generated')\n",
    "        plt.plot(t_segment, X_dataset[0][:segment_samples], \n",
    "                color='green', alpha=0.6, linewidth=1, label='Real')\n",
    "        plt.xlabel('Time [s]', fontsize=9)\n",
    "        plt.ylabel('Amplitude', fontsize=9)\n",
    "        plt.title('Detailed View (2s)', fontsize=10)\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Frequency domain comparison\n",
    "        plt.subplot(4, 3, 11)\n",
    "        from scipy.fft import fft, fftfreq\n",
    "        fft_real = np.abs(fft(X_dataset[0]))[:N//2]\n",
    "        fft_gen = np.abs(fft(generated_samples_denorm[0]))[:N//2]\n",
    "        freqs = fftfreq(N, ts)[:N//2]\n",
    "        plt.semilogy(freqs, fft_real, label='Real', alpha=0.7, linewidth=1)\n",
    "        plt.semilogy(freqs, fft_gen, label='Generated', alpha=0.7, linewidth=1)\n",
    "        plt.xlabel('Frequency [Hz]', fontsize=9)\n",
    "        plt.ylabel('Magnitude (log)', fontsize=9)\n",
    "        plt.title('Frequency Spectrum', fontsize=10)\n",
    "        plt.xlim([0, 50])\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.grid(alpha=0.3)\n",
    "\n",
    "        # Statistical comparison\n",
    "        plt.subplot(4, 3, 12)\n",
    "        real_mean = X_dataset.mean(axis=1)\n",
    "        gen_mean = generated_samples_denorm.mean(axis=1)\n",
    "        plt.hist(real_mean, bins=30, alpha=0.5, label='Real Mean', density=True)\n",
    "        plt.hist(gen_mean, bins=30, alpha=0.5, label='Gen Mean', density=True)\n",
    "        plt.xlabel('Mean Amplitude', fontsize=9)\n",
    "        plt.ylabel('Density', fontsize=9)\n",
    "        plt.title('Distribution Comparison', fontsize=10)\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Finished!\")\n",
    "print(f\"Best Wasserstein Distance: {best_w_distance:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== COMPREHENSIVE EVALUATION METRICS FOR ECG SIGNALS ==========\n",
    "from scipy.stats import pearsonr, wasserstein_distance as wd, ks_2samp\n",
    "from scipy.signal import correlate, find_peaks\n",
    "from scipy.fft import fft, fftfreq\n",
    "\n",
    "def calculate_ecg_metrics(real_data, generated_data, fs=360):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics to evaluate ECG quality\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # 1. Statistical Metrics\n",
    "    metrics['MSE'] = np.mean((real_data - generated_data) ** 2)\n",
    "    metrics['MAE'] = np.mean(np.abs(real_data - generated_data))\n",
    "    metrics['RMSE'] = np.sqrt(metrics['MSE'])\n",
    "    \n",
    "    # 2. Correlation Metrics\n",
    "    corr, p_val = pearsonr(real_data.flatten(), generated_data.flatten())\n",
    "    metrics['Pearson_Correlation'] = corr\n",
    "    metrics['Pearson_P_Value'] = p_val\n",
    "    \n",
    "    # Cross-correlation\n",
    "    cross_corr = correlate(real_data.flatten(), generated_data.flatten(), mode='valid')\n",
    "    metrics['Max_Cross_Correlation'] = np.max(cross_corr) / (np.linalg.norm(real_data) * np.linalg.norm(generated_data))\n",
    "    \n",
    "    # 3. Distribution Similarity\n",
    "    ks_stat, ks_p = ks_2samp(real_data.flatten(), generated_data.flatten())\n",
    "    metrics['KS_Statistic'] = ks_stat\n",
    "    metrics['KS_P_Value'] = ks_p\n",
    "    \n",
    "    # Wasserstein distance\n",
    "    metrics['Wasserstein_Distance'] = wd(real_data.flatten(), generated_data.flatten())\n",
    "    \n",
    "    # 4. Frequency Domain Analysis\n",
    "    fft_real = np.abs(fft(real_data.flatten()))\n",
    "    fft_gen = np.abs(fft(generated_data.flatten()))\n",
    "    \n",
    "    # Frequency similarity (correlation of spectra)\n",
    "    freq_corr, _ = pearsonr(fft_real, fft_gen)\n",
    "    metrics['Frequency_Correlation'] = freq_corr\n",
    "    \n",
    "    # Power spectral density comparison\n",
    "    psd_real = fft_real ** 2\n",
    "    psd_gen = fft_gen ** 2\n",
    "    metrics['PSD_MSE'] = np.mean((psd_real - psd_gen) ** 2)\n",
    "    \n",
    "    # 5. Temporal Coherence\n",
    "    # Measure autocorrelation similarity\n",
    "    autocorr_real = correlate(real_data.flatten(), real_data.flatten(), mode='full')\n",
    "    autocorr_gen = correlate(generated_data.flatten(), generated_data.flatten(), mode='full')\n",
    "    autocorr_real = autocorr_real[len(autocorr_real)//2:][:1000]\n",
    "    autocorr_gen = autocorr_gen[len(autocorr_gen)//2:][:1000]\n",
    "    metrics['Autocorrelation_Similarity'] = pearsonr(autocorr_real, autocorr_gen)[0]\n",
    "    \n",
    "    # 6. ECG-Specific Metrics (R-peak detection)\n",
    "    try:\n",
    "        # Detect R-peaks in real data\n",
    "        real_peaks, _ = find_peaks(real_data.flatten(), distance=fs*0.6, prominence=0.3)\n",
    "        gen_peaks, _ = find_peaks(generated_data.flatten(), distance=fs*0.6, prominence=0.3)\n",
    "        \n",
    "        # Heart rate estimation\n",
    "        if len(real_peaks) > 1:\n",
    "            real_hr = 60 / (np.mean(np.diff(real_peaks)) / fs)\n",
    "            metrics['Real_Heart_Rate'] = real_hr\n",
    "        \n",
    "        if len(gen_peaks) > 1:\n",
    "            gen_hr = 60 / (np.mean(np.diff(gen_peaks)) / fs)\n",
    "            metrics['Generated_Heart_Rate'] = gen_hr\n",
    "            metrics['Heart_Rate_Difference'] = abs(real_hr - gen_hr) if len(real_peaks) > 1 else np.nan\n",
    "        \n",
    "        metrics['Real_R_Peaks'] = len(real_peaks)\n",
    "        metrics['Generated_R_Peaks'] = len(gen_peaks)\n",
    "        metrics['R_Peak_Count_Difference'] = abs(len(real_peaks) - len(gen_peaks))\n",
    "    except:\n",
    "        metrics['R_Peak_Detection'] = 'Failed'\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_detailed_comparison(real_samples, gen_samples, t, fs=360):\n",
    "    \"\"\"Create detailed visualization comparing real and generated ECG signals\"\"\"\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    # Plot 1: Overlaid signals\n",
    "    plt.subplot(3, 3, 1)\n",
    "    for i in range(min(3, len(gen_samples))):\n",
    "        plt.plot(t, gen_samples[i], alpha=0.5, linewidth=0.5, label=f'Gen {i+1}')\n",
    "    for i in range(min(3, len(real_samples))):\n",
    "        plt.plot(t, real_samples[i], alpha=0.5, linewidth=0.5, linestyle='--', label=f'Real {i+1}')\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Signal Overlay')\n",
    "    plt.legend(fontsize=7)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xlim([0, 3])\n",
    "    \n",
    "    # Plot 2: Average signals\n",
    "    plt.subplot(3, 3, 2)\n",
    "    plt.plot(t, real_samples.mean(axis=0), color='green', linewidth=1.5, label='Real Avg', alpha=0.8)\n",
    "    plt.fill_between(t, \n",
    "                     real_samples.mean(axis=0) - real_samples.std(axis=0),\n",
    "                     real_samples.mean(axis=0) + real_samples.std(axis=0),\n",
    "                     color='green', alpha=0.2)\n",
    "    plt.plot(t, gen_samples.mean(axis=0), color='blue', linewidth=1.5, label='Gen Avg', alpha=0.8)\n",
    "    plt.fill_between(t, \n",
    "                     gen_samples.mean(axis=0) - gen_samples.std(axis=0),\n",
    "                     gen_samples.mean(axis=0) + gen_samples.std(axis=0),\n",
    "                     color='blue', alpha=0.2)\n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Mean Â± Std')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.xlim([0, 3])\n",
    "    \n",
    "    # Plot 3: Power Spectral Density\n",
    "    plt.subplot(3, 3, 3)\n",
    "    freqs = fftfreq(len(t), 1/fs)[:len(t)//2]\n",
    "    psd_real = np.mean([np.abs(fft(sig))**2 for sig in real_samples], axis=0)[:len(t)//2]\n",
    "    psd_gen = np.mean([np.abs(fft(sig))**2 for sig in gen_samples], axis=0)[:len(t)//2]\n",
    "    plt.semilogy(freqs, psd_real, label='Real', alpha=0.7, linewidth=1.5)\n",
    "    plt.semilogy(freqs, psd_gen, label='Generated', alpha=0.7, linewidth=1.5)\n",
    "    plt.xlabel('Frequency [Hz]')\n",
    "    plt.ylabel('Power Spectral Density')\n",
    "    plt.title('PSD Comparison')\n",
    "    plt.xlim([0, 50])\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 4-6: Distribution comparisons\n",
    "    metrics_to_plot = ['mean', 'std', 'max']\n",
    "    for idx, metric in enumerate(metrics_to_plot):\n",
    "        plt.subplot(3, 3, 4+idx)\n",
    "        if metric == 'mean':\n",
    "            real_vals = real_samples.mean(axis=1)\n",
    "            gen_vals = gen_samples.mean(axis=1)\n",
    "        elif metric == 'std':\n",
    "            real_vals = real_samples.std(axis=1)\n",
    "            gen_vals = gen_samples.std(axis=1)\n",
    "        else:\n",
    "            real_vals = real_samples.max(axis=1)\n",
    "            gen_vals = gen_samples.max(axis=1)\n",
    "        \n",
    "        plt.hist(real_vals, bins=20, alpha=0.5, label='Real', density=True, color='green')\n",
    "        plt.hist(gen_vals, bins=20, alpha=0.5, label='Generated', density=True, color='blue')\n",
    "        plt.xlabel(f'{metric.capitalize()} Value')\n",
    "        plt.ylabel('Density')\n",
    "        plt.title(f'{metric.capitalize()} Distribution')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 7: Autocorrelation\n",
    "    plt.subplot(3, 3, 7)\n",
    "    lag_max = int(2 * fs)  # 2 seconds\n",
    "    autocorr_real = correlate(real_samples[0], real_samples[0], mode='full')\n",
    "    autocorr_real = autocorr_real[len(autocorr_real)//2:][:lag_max]\n",
    "    autocorr_gen = correlate(gen_samples[0], gen_samples[0], mode='full')\n",
    "    autocorr_gen = autocorr_gen[len(autocorr_gen)//2:][:lag_max]\n",
    "    lags = np.arange(lag_max) / fs\n",
    "    plt.plot(lags, autocorr_real / autocorr_real[0], label='Real', linewidth=1.5, alpha=0.7)\n",
    "    plt.plot(lags, autocorr_gen / autocorr_gen[0], label='Generated', linewidth=1.5, alpha=0.7)\n",
    "    plt.xlabel('Lag [s]')\n",
    "    plt.ylabel('Normalized Autocorrelation')\n",
    "    plt.title('Autocorrelation Function')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 8: R-peak detection visualization\n",
    "    plt.subplot(3, 3, 8)\n",
    "    signal_segment = real_samples[0][:int(3*fs)]\n",
    "    t_segment = t[:int(3*fs)]\n",
    "    peaks_real, _ = find_peaks(signal_segment, distance=fs*0.6, prominence=0.3)\n",
    "    plt.plot(t_segment, signal_segment, color='green', linewidth=1, alpha=0.7, label='Real')\n",
    "    plt.plot(t_segment[peaks_real], signal_segment[peaks_real], 'ro', markersize=8, label='R-peaks')\n",
    "    \n",
    "    signal_segment_gen = gen_samples[0][:int(3*fs)]\n",
    "    peaks_gen, _ = find_peaks(signal_segment_gen, distance=fs*0.6, prominence=0.3)\n",
    "    plt.plot(t_segment, signal_segment_gen, color='blue', linewidth=1, alpha=0.7, label='Generated')\n",
    "    plt.plot(t_segment[peaks_gen], signal_segment_gen[peaks_gen], 'bs', markersize=8, label='Detected peaks')\n",
    "    \n",
    "    plt.xlabel('Time [s]')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('R-Peak Detection')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 9: Q-Q plot\n",
    "    plt.subplot(3, 3, 9)\n",
    "    from scipy import stats\n",
    "    real_sorted = np.sort(real_samples.flatten())\n",
    "    gen_sorted = np.sort(gen_samples.flatten())\n",
    "    # Subsample for clarity\n",
    "    n_points = min(1000, len(real_sorted))\n",
    "    indices = np.linspace(0, len(real_sorted)-1, n_points, dtype=int)\n",
    "    plt.scatter(real_sorted[indices], gen_sorted[indices], alpha=0.3, s=1)\n",
    "    plt.plot([real_sorted.min(), real_sorted.max()], \n",
    "             [real_sorted.min(), real_sorted.max()], \n",
    "             'r--', linewidth=2, label='Perfect match')\n",
    "    plt.xlabel('Real Quantiles')\n",
    "    plt.ylabel('Generated Quantiles')\n",
    "    plt.title('Q-Q Plot')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ========== EVALUATION ==========\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPREHENSIVE ECG EVALUATION\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "generator.eval()\n",
    "num_samples = 200  # More samples for better statistics\n",
    "\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(num_samples, noise_dim).to(device)\n",
    "    generated_samples = generator(noise).cpu().numpy()\n",
    "    # Denormalize\n",
    "    generated_samples = (generated_samples + 1) / 2 * (X_max - X_min) + X_min\n",
    "\n",
    "# Select real samples\n",
    "real_samples = X_dataset[:num_samples]\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "print(\"Calculating metrics...\")\n",
    "metrics = calculate_ecg_metrics(real_samples, generated_samples, fs=fs)\n",
    "\n",
    "print(\"\\nðŸ“Š EVALUATION RESULTS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nðŸ”¢ Statistical Metrics:\")\n",
    "for key in ['MSE', 'MAE', 'RMSE']:\n",
    "    if key in metrics:\n",
    "        print(f\"  {key}: {metrics[key]:.6f}\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Correlation Metrics:\")\n",
    "for key in ['Pearson_Correlation', 'Max_Cross_Correlation', 'Frequency_Correlation', 'Autocorrelation_Similarity']:\n",
    "    if key in metrics:\n",
    "        print(f\"  {key}: {metrics[key]:.6f}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Distribution Metrics:\")\n",
    "for key in ['KS_Statistic', 'Wasserstein_Distance']:\n",
    "    if key in metrics:\n",
    "        print(f\"  {key}: {metrics[key]:.6f}\")\n",
    "\n",
    "print(\"\\nðŸ’“ ECG-Specific Metrics:\")\n",
    "for key in ['Real_Heart_Rate', 'Generated_Heart_Rate', 'Heart_Rate_Difference', \n",
    "            'Real_R_Peaks', 'Generated_R_Peaks', 'R_Peak_Count_Difference']:\n",
    "    if key in metrics:\n",
    "        print(f\"  {key}: {metrics[key]:.2f}\")\n",
    "\n",
    "print(\"\\nðŸ“‰ Statistical Comparison:\")\n",
    "print(f\"  Real data     - Mean: {real_samples.mean():.4f}, Std: {real_samples.std():.4f}\")\n",
    "print(f\"  Generated     - Mean: {generated_samples.mean():.4f}, Std: {generated_samples.std():.4f}\")\n",
    "print(f\"  Real data     - Min: {real_samples.min():.4f}, Max: {real_samples.max():.4f}\")\n",
    "print(f\"  Generated     - Min: {generated_samples.min():.4f}, Max: {generated_samples.max():.4f}\")\n",
    "\n",
    "# Create detailed visualization\n",
    "plot_detailed_comparison(real_samples[:50], generated_samples[:50], t, fs=fs)\n",
    "\n",
    "# Save final model and results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Saving models and results...\")\n",
    "torch.save({\n",
    "    'generator': generator.state_dict(),\n",
    "    'discriminator': discriminator.state_dict(),\n",
    "    'g_optimizer': g_optimizer.state_dict(),\n",
    "    'd_optimizer': d_optimizer.state_dict(),\n",
    "    'epoch': num_epochs,\n",
    "    'metrics': metrics,\n",
    "    'architecture': architecture,\n",
    "    'noise_dim': noise_dim,\n",
    "    'lstm_hidden': lstm_hidden,\n",
    "    'lstm_layers': lstm_layers\n",
    "}, 'ecg_cnn_lstm_wgan_gp_final.pth')\n",
    "print(\"âœ… Model saved to: ecg_cnn_lstm_wgan_gp_final.pth\")\n",
    "\n",
    "# Save metrics to CSV\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv('ecg_lstm_gan_metrics.csv', index=False)\n",
    "print(\"âœ… Metrics saved to: ecg_lstm_gan_metrics.csv\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== GENERATE SYNTHETIC ECG DATASET ==========\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GENERATING SYNTHETIC ECG DATASET\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Number of synthetic signals to generate\n",
    "num_synthetic = 500\n",
    "\n",
    "print(f\"Generating {num_synthetic} synthetic ECG signals...\")\n",
    "generator.eval()\n",
    "synthetic_signals = []\n",
    "\n",
    "batch_size_gen = 50\n",
    "with torch.no_grad():\n",
    "    for i in range(0, num_synthetic, batch_size_gen):\n",
    "        batch_size_current = min(batch_size_gen, num_synthetic - i)\n",
    "        noise = torch.randn(batch_size_current, noise_dim).to(device)\n",
    "        generated = generator(noise).cpu().numpy()\n",
    "        # Denormalize\n",
    "        generated = (generated + 1) / 2 * (X_max - X_min) + X_min\n",
    "        synthetic_signals.append(generated)\n",
    "        \n",
    "        if (i + batch_size_current) % 100 == 0:\n",
    "            print(f\"  Generated {i + batch_size_current}/{num_synthetic} signals...\")\n",
    "\n",
    "synthetic_signals = np.vstack(synthetic_signals)\n",
    "print(f\"âœ… Successfully generated {synthetic_signals.shape[0]} signals\")\n",
    "print(f\"   Shape: {synthetic_signals.shape}\")\n",
    "\n",
    "# Visualize a grid of synthetic signals\n",
    "print(\"\\nVisualizing synthetic signals...\")\n",
    "fig, axes = plt.subplots(5, 4, figsize=(20, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(20):\n",
    "    ax = axes[i]\n",
    "    ax.plot(t[:int(2*fs)], synthetic_signals[i][:int(2*fs)], color='blue', linewidth=0.8, alpha=0.8)\n",
    "    ax.set_xlabel('Time [s]', fontsize=8)\n",
    "    ax.set_ylabel('Amplitude', fontsize=8)\n",
    "    ax.set_title(f'Synthetic ECG #{i+1}', fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.set_xlim([0, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Generated Synthetic ECG Signals (2-second segments)', fontsize=14, y=1.001)\n",
    "plt.show()\n",
    "\n",
    "# Save synthetic dataset\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Saving synthetic dataset...\")\n",
    "\n",
    "# Save as numpy array\n",
    "np.save('synthetic_ecg_signals.npy', synthetic_signals)\n",
    "print(\"âœ… Saved as: synthetic_ecg_signals.npy\")\n",
    "\n",
    "# Save as pickle (same format as original dataset)\n",
    "synthetic_dataset = {\n",
    "    'NSR_synthetic': synthetic_signals,\n",
    "    'metadata': {\n",
    "        'num_signals': synthetic_signals.shape[0],\n",
    "        'signal_length': synthetic_signals.shape[1],\n",
    "        'sampling_rate': fs,\n",
    "        'duration_seconds': synthetic_signals.shape[1] / fs,\n",
    "        'generation_method': 'CNN-LSTM WGAN-GP',\n",
    "        'architecture': architecture,\n",
    "        'noise_dim': noise_dim,\n",
    "        'lstm_hidden': lstm_hidden,\n",
    "        'lstm_layers': lstm_layers\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('synthetic_ecg_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(synthetic_dataset, f)\n",
    "print(\"âœ… Saved as: synthetic_ecg_dataset.pkl\")\n",
    "\n",
    "# Quality assessment of synthetic dataset\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SYNTHETIC DATASET QUALITY ASSESSMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compare distributions\n",
    "print(\"\\nðŸ“Š Statistical Properties:\")\n",
    "print(f\"  Real dataset:\")\n",
    "print(f\"    Mean: {X_dataset.mean():.4f} Â± {X_dataset.std():.4f}\")\n",
    "print(f\"    Range: [{X_dataset.min():.4f}, {X_dataset.max():.4f}]\")\n",
    "print(f\"  Synthetic dataset:\")\n",
    "print(f\"    Mean: {synthetic_signals.mean():.4f} Â± {synthetic_signals.std():.4f}\")\n",
    "print(f\"    Range: [{synthetic_signals.min():.4f}, {synthetic_signals.max():.4f}]\")\n",
    "\n",
    "# Heart rate analysis for synthetic signals\n",
    "print(\"\\nðŸ’“ Heart Rate Analysis (sample of 50 signals):\")\n",
    "heart_rates_synthetic = []\n",
    "for i in range(min(50, len(synthetic_signals))):\n",
    "    try:\n",
    "        peaks, _ = find_peaks(synthetic_signals[i], distance=fs*0.6, prominence=0.3)\n",
    "        if len(peaks) > 1:\n",
    "            hr = 60 / (np.mean(np.diff(peaks)) / fs)\n",
    "            if 40 < hr < 120:  # Physiologically reasonable range\n",
    "                heart_rates_synthetic.append(hr)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "if heart_rates_synthetic:\n",
    "    print(f\"  Average HR: {np.mean(heart_rates_synthetic):.1f} Â± {np.std(heart_rates_synthetic):.1f} bpm\")\n",
    "    print(f\"  Range: [{np.min(heart_rates_synthetic):.1f}, {np.max(heart_rates_synthetic):.1f}] bpm\")\n",
    "    print(f\"  Valid signals: {len(heart_rates_synthetic)}/50\")\n",
    "\n",
    "# Final comparison plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Distribution of means\n",
    "axes[0, 0].hist(X_dataset.mean(axis=1), bins=40, alpha=0.5, label='Real', density=True, color='green')\n",
    "axes[0, 0].hist(synthetic_signals.mean(axis=1), bins=40, alpha=0.5, label='Synthetic', density=True, color='blue')\n",
    "axes[0, 0].set_xlabel('Mean Amplitude')\n",
    "axes[0, 0].set_ylabel('Density')\n",
    "axes[0, 0].set_title('Distribution of Signal Means')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Distribution of stds\n",
    "axes[0, 1].hist(X_dataset.std(axis=1), bins=40, alpha=0.5, label='Real', density=True, color='green')\n",
    "axes[0, 1].hist(synthetic_signals.std(axis=1), bins=40, alpha=0.5, label='Synthetic', density=True, color='blue')\n",
    "axes[0, 1].set_xlabel('Standard Deviation')\n",
    "axes[0, 1].set_ylabel('Density')\n",
    "axes[0, 1].set_title('Distribution of Signal Std Dev')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Sample real vs synthetic\n",
    "axes[1, 0].plot(t[:int(3*fs)], X_dataset[0][:int(3*fs)], color='green', linewidth=1.2, alpha=0.8, label='Real')\n",
    "axes[1, 0].plot(t[:int(3*fs)], synthetic_signals[0][:int(3*fs)], color='blue', linewidth=1.2, alpha=0.8, label='Synthetic')\n",
    "axes[1, 0].set_xlabel('Time [s]')\n",
    "axes[1, 0].set_ylabel('Amplitude')\n",
    "axes[1, 0].set_title('Sample Comparison (3 seconds)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Average PSD\n",
    "axes[1, 1].set_title('Average Power Spectral Density')\n",
    "freqs = fftfreq(N, ts)[:N//2]\n",
    "psd_real_avg = np.mean([np.abs(fft(sig))**2 for sig in X_dataset[:50]], axis=0)[:N//2]\n",
    "psd_syn_avg = np.mean([np.abs(fft(sig))**2 for sig in synthetic_signals[:50]], axis=0)[:N//2]\n",
    "axes[1, 1].semilogy(freqs, psd_real_avg, label='Real', linewidth=1.5, alpha=0.8, color='green')\n",
    "axes[1, 1].semilogy(freqs, psd_syn_avg, label='Synthetic', linewidth=1.5, alpha=0.8, color='blue')\n",
    "axes[1, 1].set_xlabel('Frequency [Hz]')\n",
    "axes[1, 1].set_ylabel('Power')\n",
    "axes[1, 1].set_xlim([0, 50])\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"âœ… PROCESS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nðŸ“ Generated Files:\")\n",
    "print(\"  1. ecg_cnn_lstm_wgan_gp_final.pth - Trained model\")\n",
    "print(\"  2. best_ecg_lstm_gan.pth - Best model checkpoint\")\n",
    "print(\"  3. synthetic_ecg_signals.npy - Synthetic signals (numpy)\")\n",
    "print(\"  4. synthetic_ecg_dataset.pkl - Synthetic dataset (pickle)\")\n",
    "print(\"  5. ecg_lstm_gan_metrics.csv - Evaluation metrics\")\n",
    "print(\"\\nðŸŽ‰ Ready for downstream tasks (classification, augmentation, etc.)\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-wsl2-conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
