{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3683c11a",
   "metadata": {},
   "source": [
    "## 1. Importaci√≥n de Librer√≠as <a id='1-importacion'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e5ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as est√°ndar\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# Procesamiento de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db76f78f",
   "metadata": {},
   "source": [
    "## 2. Carga y Exploraci√≥n de Datos <a id='2-datos'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ecc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset\n",
    "path_db = \"./ECG_DATASET/dataset_ekg.pkl\"\n",
    "\n",
    "with open(path_db, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "print(\"Clases disponibles:\", list(dataset.keys()))\n",
    "print(\"\\nForma de cada clase:\")\n",
    "for key, value in dataset.items():\n",
    "    print(f\"  {key:15s}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af39c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar se√±ales de ejemplo\n",
    "class_names = list(dataset.keys())\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, class_name in enumerate(class_names[:16]):\n",
    "    signal = dataset[class_name][0]\n",
    "    axes[idx].plot(signal, linewidth=0.8)\n",
    "    axes[idx].set_title(f\"{class_name}\", fontsize=10)\n",
    "    axes[idx].set_xlabel(\"Muestra\")\n",
    "    axes[idx].set_ylabel(\"Amplitud\")\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Ejemplos de Se√±ales ECG por Clase\", fontsize=14, y=1.001)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01fcc2",
   "metadata": {},
   "source": [
    "## 3. Preparaci√≥n del Dataset <a id='3-dataset'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f960c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para se√±ales ECG\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dict, class_to_idx, normalize=True):\n",
    "        self.signals = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for class_name, signals in data_dict.items():\n",
    "            class_idx = class_to_idx[class_name]\n",
    "            for signal in signals:\n",
    "                self.signals.append(signal)\n",
    "                self.labels.append(class_idx)\n",
    "        \n",
    "        self.signals = np.array(self.signals, dtype=np.float32)\n",
    "        self.labels = np.array(self.labels, dtype=np.int64)\n",
    "        \n",
    "        if normalize:\n",
    "            for i in range(len(self.signals)):\n",
    "                min_val = self.signals[i].min()\n",
    "                max_val = self.signals[i].max()\n",
    "                if max_val > min_val:\n",
    "                    self.signals[i] = 2 * (self.signals[i] - min_val) / (max_val - min_val) - 1\n",
    "        \n",
    "        print(f\"Dataset: {len(self.signals)} se√±ales, {len(np.unique(self.labels))} clases\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.signals)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        signal = torch.FloatTensor(self.signals[idx]).unsqueeze(0)\n",
    "        label = torch.LongTensor([self.labels[idx]])\n",
    "        return signal, label\n",
    "\n",
    "# Crear dataset\n",
    "class_to_idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "idx_to_class = {idx: name for name, idx in class_to_idx.items()}\n",
    "\n",
    "ecg_dataset = ECGDataset(dataset, class_to_idx, normalize=True)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(ecg_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"N√∫mero de batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb554f39",
   "metadata": {},
   "source": [
    "## 4. Arquitectura del Modelo <a id='4-arquitectura'></a>\n",
    "\n",
    "### 4.1 Generador Mejorado (Conv + LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67acb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedGenerator(nn.Module):\n",
    "    \"\"\"Generador mejorado: ConvTranspose + LSTM + Residual\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, num_classes, seq_length, embedding_dim=50):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Embedding de clase\n",
    "        self.label_embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "        input_dim = latent_dim + embedding_dim\n",
    "        \n",
    "        # Proyecci√≥n inicial\n",
    "        self.fc_project = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256 * 14),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        # ConvTranspose1D progresivas\n",
    "        self.deconv1 = nn.ConvTranspose1d(256, 128, kernel_size=25, stride=4, padding=11)\n",
    "        self.deconv2 = nn.ConvTranspose1d(128, 64, kernel_size=25, stride=4, padding=11)\n",
    "        self.deconv3 = nn.ConvTranspose1d(64, 32, kernel_size=25, stride=4, padding=11)\n",
    "        self.deconv4 = nn.ConvTranspose1d(32, 16, kernel_size=25, stride=4, padding=11)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.bn4 = nn.BatchNorm1d(16)\n",
    "        \n",
    "        # LSTM para coherencia temporal\n",
    "        self.lstm = nn.LSTM(16, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Capas finales con residual\n",
    "        self.conv_res1 = nn.Conv1d(64, 32, kernel_size=1)\n",
    "        self.conv_res2 = nn.Conv1d(32, 16, kernel_size=1)\n",
    "        self.conv_out = nn.Conv1d(16, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, noise, labels):\n",
    "        batch_size = noise.size(0)\n",
    "        \n",
    "        # Embedding + concatenar\n",
    "        label_emb = self.label_embedding(labels.squeeze(1))\n",
    "        x = torch.cat([noise, label_emb], dim=1)\n",
    "        \n",
    "        # Proyecci√≥n\n",
    "        x = self.fc_project(x).view(batch_size, 256, 14)\n",
    "        \n",
    "        # Deconvoluciones\n",
    "        x = F.leaky_relu(self.bn1(self.deconv1(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn2(self.deconv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn3(self.deconv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn4(self.deconv4(x)), 0.2)\n",
    "        \n",
    "        # LSTM\n",
    "        x_temp = x.transpose(1, 2)\n",
    "        x_temp, _ = self.lstm(x_temp)\n",
    "        x_lstm = x_temp.transpose(1, 2)\n",
    "        \n",
    "        # Residual\n",
    "        x_res = F.leaky_relu(self.conv_res1(x_lstm), 0.2)\n",
    "        x_res = self.conv_res2(x_res)\n",
    "        x = x + x_res[:, :, :x.size(2)]\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "        x = torch.tanh(self.conv_out(x))\n",
    "        \n",
    "        # Ajustar a longitud exacta\n",
    "        x = F.interpolate(x, size=self.seq_length, mode='linear', align_corners=False)\n",
    "        \n",
    "        return x\n",
    "\n",
    "print(\"‚úì Generador mejorado definido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483ac37d",
   "metadata": {},
   "source": [
    "### 4.2 Discriminador Mejorado (Conv + LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904e388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedDiscriminator(nn.Module):\n",
    "    \"\"\"Discriminador mejorado: Conv1D + Instance Norm + LSTM\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, seq_length, embedding_dim=50):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding de clase\n",
    "        self.label_embedding = nn.Embedding(num_classes, embedding_dim)\n",
    "        self.label_proj = nn.Linear(embedding_dim, seq_length)\n",
    "        \n",
    "        # Convoluciones\n",
    "        self.conv1 = nn.Conv1d(2, 32, kernel_size=25, stride=4, padding=12)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=25, stride=4, padding=12)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=25, stride=4, padding=12)\n",
    "        self.conv4 = nn.Conv1d(128, 256, kernel_size=25, stride=4, padding=12)\n",
    "        \n",
    "        # Instance Norm\n",
    "        self.in1 = nn.InstanceNorm1d(32)\n",
    "        self.in2 = nn.InstanceNorm1d(64)\n",
    "        self.in3 = nn.InstanceNorm1d(128)\n",
    "        self.in4 = nn.InstanceNorm1d(256)\n",
    "        \n",
    "        # LSTM bidireccional\n",
    "        self.lstm = nn.LSTM(256, 128, num_layers=2, batch_first=True, \n",
    "                           bidirectional=True, dropout=0.3)\n",
    "        \n",
    "        # Clasificaci√≥n\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, signal, labels):\n",
    "        # Embedding + proyecci√≥n\n",
    "        label_emb = self.label_embedding(labels.squeeze(1))\n",
    "        label_proj = self.label_proj(label_emb).unsqueeze(1)\n",
    "        \n",
    "        # Concatenar\n",
    "        x = torch.cat([signal, label_proj], dim=1)\n",
    "        \n",
    "        # Convoluciones\n",
    "        x = F.leaky_relu(self.in1(self.conv1(x)), 0.2)\n",
    "        x = F.leaky_relu(self.in2(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.in3(self.conv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.in4(self.conv4(x)), 0.2)\n",
    "        \n",
    "        # LSTM\n",
    "        x = x.transpose(1, 2)\n",
    "        lstm_out, (h_n, _) = self.lstm(x)\n",
    "        last_hidden = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "        \n",
    "        # Clasificaci√≥n\n",
    "        validity = self.fc(last_hidden)\n",
    "        \n",
    "        return validity\n",
    "\n",
    "print(\"‚úì Discriminador mejorado definido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae076ba6",
   "metadata": {},
   "source": [
    "## 5. Configuraci√≥n de Entrenamiento <a id='5-configuracion'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b0850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Dispositivo: {device}\")\n",
    "\n",
    "# Hiperpar√°metros\n",
    "latent_dim = 100\n",
    "num_classes = len(class_names)\n",
    "seq_length = 3600\n",
    "embedding_dim = 50\n",
    "\n",
    "lr_g = 0.0001\n",
    "lr_d = 0.0001\n",
    "num_epochs = 2000\n",
    "sample_interval = 100\n",
    "\n",
    "print(f\"\\nHiperpar√°metros:\")\n",
    "print(f\"  Latent dim: {latent_dim}\")\n",
    "print(f\"  Clases: {num_classes}\")\n",
    "print(f\"  Seq length: {seq_length}\")\n",
    "print(f\"  LR G/D: {lr_g} / {lr_d}\")\n",
    "print(f\"  √âpocas: {num_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelos\n",
    "generator = ImprovedGenerator(\n",
    "    latent_dim=latent_dim,\n",
    "    num_classes=num_classes,\n",
    "    seq_length=seq_length,\n",
    "    embedding_dim=embedding_dim\n",
    ").to(device)\n",
    "\n",
    "discriminator = ImprovedDiscriminator(\n",
    "    num_classes=num_classes,\n",
    "    seq_length=seq_length,\n",
    "    embedding_dim=embedding_dim\n",
    ").to(device)\n",
    "\n",
    "print(f\"Generador: {sum(p.numel() for p in generator.parameters()):,} par√°metros\")\n",
    "print(f\"Discriminador: {sum(p.numel() for p in discriminator.parameters()):,} par√°metros\")\n",
    "\n",
    "# Optimizadores\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "\n",
    "# Schedulers\n",
    "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=500, gamma=0.95)\n",
    "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=500, gamma=0.95)\n",
    "\n",
    "# Loss\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "print(\"\\n‚úì Modelos y optimizadores inicializados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80072860",
   "metadata": {},
   "source": [
    "### 5.1 Funci√≥n de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e8d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(generator, discriminator, dataloader, optimizer_G, optimizer_D, \n",
    "                adversarial_loss, device):\n",
    "    \"\"\"Entrenamiento mejorado con m√∫ltiples losses\"\"\"\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    epoch_g_loss = 0.0\n",
    "    epoch_d_loss = 0.0\n",
    "    epoch_d_real_acc = 0.0\n",
    "    epoch_d_fake_acc = 0.0\n",
    "    \n",
    "    # Label smoothing\n",
    "    real_label = 0.9\n",
    "    fake_label = 0.1\n",
    "    \n",
    "    for i, (real_signals, labels) in enumerate(dataloader):\n",
    "        batch_size = real_signals.size(0)\n",
    "        real_signals = real_signals.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        valid = torch.full((batch_size, 1), real_label, device=device)\n",
    "        fake_labels_d = torch.full((batch_size, 1), fake_label, device=device)\n",
    "        \n",
    "        # Entrenar Discriminador\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        real_pred = discriminator(real_signals, labels)\n",
    "        d_real_loss = adversarial_loss(real_pred, valid)\n",
    "        \n",
    "        z = torch.randn(batch_size, latent_dim, device=device)\n",
    "        gen_labels = torch.randint(0, num_classes, (batch_size, 1), device=device)\n",
    "        gen_signals = generator(z, gen_labels)\n",
    "        fake_pred = discriminator(gen_signals.detach(), gen_labels)\n",
    "        d_fake_loss = adversarial_loss(fake_pred, fake_labels_d)\n",
    "        \n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Entrenar Generador (2 veces)\n",
    "        for _ in range(2):\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size, latent_dim, device=device)\n",
    "            gen_labels = torch.randint(0, num_classes, (batch_size, 1), device=device)\n",
    "            gen_signals = generator(z, gen_labels)\n",
    "            \n",
    "            validity = discriminator(gen_signals, gen_labels)\n",
    "            g_adv_loss = adversarial_loss(validity, torch.ones((batch_size, 1), device=device))\n",
    "            \n",
    "            # Feature matching\n",
    "            real_mean = real_signals.mean(dim=2, keepdim=True)\n",
    "            real_std = real_signals.std(dim=2, keepdim=True)\n",
    "            fake_mean = gen_signals.mean(dim=2, keepdim=True)\n",
    "            fake_std = gen_signals.std(dim=2, keepdim=True)\n",
    "            feature_loss = F.mse_loss(fake_mean, real_mean) + F.mse_loss(fake_std, real_std)\n",
    "            \n",
    "            # Frequency loss\n",
    "            real_fft = torch.fft.rfft(real_signals, dim=2)\n",
    "            fake_fft = torch.fft.rfft(gen_signals, dim=2)\n",
    "            freq_loss = F.l1_loss(torch.abs(fake_fft), torch.abs(real_fft))\n",
    "            \n",
    "            g_loss = g_adv_loss + 0.1 * feature_loss + 0.05 * freq_loss\n",
    "            \n",
    "            g_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "            optimizer_G.step()\n",
    "        \n",
    "        # M√©tricas\n",
    "        epoch_g_loss += g_loss.item()\n",
    "        epoch_d_loss += d_loss.item()\n",
    "        epoch_d_real_acc += (real_pred > 0.5).float().mean().item()\n",
    "        epoch_d_fake_acc += (fake_pred < 0.5).float().mean().item()\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "    return {\n",
    "        'g_loss': epoch_g_loss / num_batches,\n",
    "        'd_loss': epoch_d_loss / num_batches,\n",
    "        'd_real_acc': epoch_d_real_acc / num_batches,\n",
    "        'd_fake_acc': epoch_d_fake_acc / num_batches\n",
    "    }\n",
    "\n",
    "print(\"‚úì Funci√≥n de entrenamiento definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a97f0f",
   "metadata": {},
   "source": [
    "### 5.2 Funci√≥n de Visualizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649706f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_plot_signals(generator, num_classes, idx_to_class, latent_dim, \n",
    "                              seq_length, device, epoch, save_dir='generated_samples'):\n",
    "    \"\"\"Genera y visualiza se√±ales sint√©ticas\"\"\"\n",
    "    generator.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 5, figsize=(20, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for class_idx in range(min(num_classes, 20)):\n",
    "            z = torch.randn(1, latent_dim, device=device)\n",
    "            label = torch.LongTensor([[class_idx]]).to(device)\n",
    "            gen_signal = generator(z, label).cpu().numpy().squeeze()\n",
    "            \n",
    "            axes[class_idx].plot(gen_signal, linewidth=0.8)\n",
    "            axes[class_idx].set_title(f\"{idx_to_class[class_idx]}\", fontsize=10)\n",
    "            axes[class_idx].set_xlabel(\"Muestra\")\n",
    "            axes[class_idx].set_ylabel(\"Amplitud\")\n",
    "            axes[class_idx].grid(alpha=0.3)\n",
    "            axes[class_idx].set_ylim(-1.5, 1.5)\n",
    "    \n",
    "    for idx in range(num_classes, 20):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Se√±ales Generadas - Epoch {epoch}\", fontsize=14, y=1.001)\n",
    "    \n",
    "    save_path = os.path.join(save_dir, f'epoch_{epoch:04d}.png')\n",
    "    plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"  ‚Üí Muestras guardadas en {save_path}\")\n",
    "    generator.train()\n",
    "\n",
    "print(\"‚úì Funci√≥n de visualizaci√≥n definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e207be",
   "metadata": {},
   "source": [
    "## 6. Entrenamiento <a id='6-entrenamiento'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a254df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historial\n",
    "history = {\n",
    "    'g_loss': [],\n",
    "    'd_loss': [],\n",
    "    'd_real_acc': [],\n",
    "    'd_fake_acc': []\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ENTRENAMIENTO CGAN-LSTM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    metrics = train_epoch(\n",
    "        generator, discriminator, dataloader,\n",
    "        optimizer_G, optimizer_D, adversarial_loss, device\n",
    "    )\n",
    "    \n",
    "    # Guardar m√©tricas\n",
    "    for key in history.keys():\n",
    "        history[key].append(metrics[key])\n",
    "    \n",
    "    # Update LR\n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    # Imprimir\n",
    "    if (epoch + 1) % 50 == 0 or epoch == 0:\n",
    "        lr_g = optimizer_G.param_groups[0]['lr']\n",
    "        print(f\"Epoch [{epoch+1:4d}/{num_epochs}] | \"\n",
    "              f\"G: {metrics['g_loss']:.4f} | \"\n",
    "              f\"D: {metrics['d_loss']:.4f} | \"\n",
    "              f\"D_real: {metrics['d_real_acc']:.3f} | \"\n",
    "              f\"D_fake: {metrics['d_fake_acc']:.3f} | \"\n",
    "              f\"LR: {lr_g:.6f} | \"\n",
    "              f\"{epoch_time:.2f}s\")\n",
    "    \n",
    "    # Visualizar\n",
    "    if (epoch + 1) % sample_interval == 0:\n",
    "        generate_and_plot_signals(\n",
    "            generator, num_classes, idx_to_class, latent_dim,\n",
    "            seq_length, device, epoch + 1\n",
    "        )\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Entrenamiento completado en {total_time/60:.2f} minutos\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feca762e",
   "metadata": {},
   "source": [
    "## 7. Evaluaci√≥n y Visualizaci√≥n <a id='7-evaluacion'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab94f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar m√©tricas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# P√©rdidas\n",
    "axes[0, 0].plot(history['g_loss'], label='Generator Loss', linewidth=2, alpha=0.8)\n",
    "axes[0, 0].plot(history['d_loss'], label='Discriminator Loss', linewidth=2, alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('P√©rdidas')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Precisiones\n",
    "axes[0, 1].plot(history['d_real_acc'], label='D Acc Real', linewidth=2, alpha=0.8)\n",
    "axes[0, 1].plot(history['d_fake_acc'], label='D Acc Fake', linewidth=2, alpha=0.8)\n",
    "axes[0, 1].axhline(y=0.5, color='r', linestyle='--', label='Equilibrio')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].set_title('Precisi√≥n del Discriminador')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].set_ylim(0, 1)\n",
    "\n",
    "# Loss suavizado\n",
    "window = 50\n",
    "if len(history['g_loss']) >= window:\n",
    "    g_smooth = np.convolve(history['g_loss'], np.ones(window)/window, mode='valid')\n",
    "    d_smooth = np.convolve(history['d_loss'], np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    axes[1, 0].plot(g_smooth, label='G Loss', linewidth=2)\n",
    "    axes[1, 0].plot(d_smooth, label='D Loss', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].set_title(f'P√©rdidas Suavizadas (ventana={window})')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì M√©tricas visualizadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar Real vs Generado\n",
    "generator.eval()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, class_name in enumerate(class_names[:16]):\n",
    "        class_idx = class_to_idx[class_name]\n",
    "        real_signal = dataset[class_name][0]\n",
    "        \n",
    "        z = torch.randn(1, latent_dim, device=device)\n",
    "        label = torch.LongTensor([[class_idx]]).to(device)\n",
    "        gen_signal = generator(z, label).cpu().numpy().squeeze()\n",
    "        \n",
    "        axes[idx].plot(real_signal[:1000], label='Real', linewidth=1.5, alpha=0.7)\n",
    "        axes[idx].plot(gen_signal[:1000], label='Generada', linewidth=1.5, alpha=0.7)\n",
    "        axes[idx].set_title(f\"{class_name}\", fontsize=10)\n",
    "        axes[idx].set_ylim(-1.5, 1.5)\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "        axes[idx].legend(fontsize=8)\n",
    "\n",
    "plt.suptitle(\"Comparaci√≥n Real vs Generada (primeras 1000 muestras)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparison_real_vs_generated.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Comparaci√≥n completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b530fb7",
   "metadata": {},
   "source": [
    "## 8. Guardar y Cargar Modelos <a id='8-guardar'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be276473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelos\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': generator.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "    'history': history,\n",
    "    'config': {\n",
    "        'latent_dim': latent_dim,\n",
    "        'num_classes': num_classes,\n",
    "        'seq_length': seq_length,\n",
    "        'embedding_dim': embedding_dim\n",
    "    }\n",
    "}, 'models/generator_cgan_lstm.pth')\n",
    "\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': discriminator.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "}, 'models/discriminator_cgan_lstm.pth')\n",
    "\n",
    "print(\"‚úì Modelos guardados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f900af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para cargar\n",
    "def load_generator(checkpoint_path, device='cpu'):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    config = checkpoint['config']\n",
    "    \n",
    "    generator = ImprovedGenerator(\n",
    "        latent_dim=config['latent_dim'],\n",
    "        num_classes=config['num_classes'],\n",
    "        seq_length=config['seq_length'],\n",
    "        embedding_dim=config['embedding_dim']\n",
    "    ).to(device)\n",
    "    \n",
    "    generator.load_state_dict(checkpoint['model_state_dict'])\n",
    "    generator.eval()\n",
    "    \n",
    "    print(f\"Generador cargado (epoch {checkpoint['epoch']})\")\n",
    "    return generator, config\n",
    "\n",
    "# Ejemplo de uso\n",
    "def generate_ecg_for_class(generator, class_name, class_to_idx, latent_dim, \n",
    "                           num_samples=5, device='cpu'):\n",
    "    generator.eval()\n",
    "    class_idx = class_to_idx[class_name]\n",
    "    \n",
    "    signals = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            z = torch.randn(1, latent_dim, device=device)\n",
    "            label = torch.LongTensor([[class_idx]]).to(device)\n",
    "            signal = generator(z, label).cpu().numpy().squeeze()\n",
    "            signals.append(signal)\n",
    "    \n",
    "    return np.array(signals)\n",
    "\n",
    "print(\"‚úì Funciones de carga definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f932bffd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Resumen\n",
    "\n",
    "### Arquitectura Mejorada:\n",
    "- **Generador**: ConvTranspose1D + LSTM + Residual connections\n",
    "- **Discriminador**: Conv1D + Instance Norm + LSTM bidireccional\n",
    "\n",
    "### Mejoras Clave:\n",
    "1. ‚úÖ ConvTranspose1D para generaci√≥n progresiva\n",
    "2. ‚úÖ Instance Normalization (m√°s estable que BatchNorm)\n",
    "3. ‚úÖ Feature matching loss\n",
    "4. ‚úÖ Frequency domain loss\n",
    "5. ‚úÖ Label smoothing\n",
    "6. ‚úÖ Gradient clipping\n",
    "7. ‚úÖ Learning rate scheduling\n",
    "\n",
    "### Uso:\n",
    "```python\n",
    "# Generar se√±ales\n",
    "signals = generate_ecg_for_class(generator, 'NSR', class_to_idx, latent_dim, num_samples=10)\n",
    "\n",
    "# Cargar modelo guardado\n",
    "loaded_gen, config = load_generator('models/generator_cgan_lstm.pth', device)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
